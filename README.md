# ğŸŒ³ Tree of Thought Benchmarking

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![Ollama](https://img.shields.io/badge/Ollama-Latest-green.svg)](https://ollama.ai/)

A comprehensive benchmarking suite comparing Microsoft's Phi4-mini-reasoning model with Tree of Thought (ToT) prompting versus standard prompting approaches.

## ğŸ“Š Key Results

Initial benchmarking shows **measurable improvements** with Tree of Thought methodology:

- **Overall Performance**: +4.5% improvement
- **Reasoning Tasks**: +17.2% improvement ğŸš€
- **Math Problems**: +4.3% improvement
- **Logic Puzzles**: +0.5% improvement

> *Tree of Thought shows the biggest gains in complex reasoning tasks, validating the structured thinking approach.*

## ğŸ“ Project Structure

```text
ToT_Tuning/
â”œâ”€â”€ README.md                 # Project documentation
â”œâ”€â”€ QUICKSTART.md            # Quick setup guide  
â”œâ”€â”€ requirements.txt         # Python dependencies
â”œâ”€â”€ config/                  # Configuration files
â”‚   â”œâ”€â”€ model_config.yaml    # Model settings
â”‚   â””â”€â”€ benchmark_config.yaml # Benchmark parameters
â”œâ”€â”€ modelfiles/              # Ollama model definitions
â”‚   â”œâ”€â”€ phi4_tot.modelfile   # ToT-enhanced model
â”‚   â””â”€â”€ phi4_normal.modelfile # Standard model
â”œâ”€â”€ src/                     # Source code
â”‚   â”œâ”€â”€ benchmark.py         # Main benchmarking engine
â”‚   â”œâ”€â”€ model_manager.py     # Ollama model management
â”‚   â”œâ”€â”€ evaluator.py         # Response evaluation metrics
â”‚   â””â”€â”€ prompts/             # Prompt templates
â”œâ”€â”€ data/                    # Test data and results
â”‚   â”œâ”€â”€ test_datasets/       # Curated test problems
â”‚   â””â”€â”€ results/             # Benchmark outputs
â””â”€â”€ scripts/                 # Utility scripts
    â”œâ”€â”€ setup_models.py      # Model setup automation
    â”œâ”€â”€ run_benchmark.py     # Main execution
    â””â”€â”€ test_setup.py        # Project verification
```

## ğŸš€ Quick Start

1. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

2. **Install and setup Ollama:**
   ```bash
   curl -fsSL https://ollama.ai/install.sh | sh
   ollama pull phi4-mini-reasoning
   ```

3. **Setup benchmark models:**
   ```bash
   python scripts/setup_models.py
   ```

4. **Run benchmark:**
   ```bash
   python scripts/run_benchmark.py
   ```

## ğŸ¯ What This Tests

### Problem Categories
- **Math Problems**: Arithmetic, algebra, geometry
- **Logic Puzzles**: Deductive reasoning, constraint satisfaction
- **General Reasoning**: Analysis, comparison, argumentation

### Evaluation Metrics
- **Accuracy** (40%): Correctness of final answers
- **Reasoning Quality** (25%): Logical coherence and depth
- **Completeness** (20%): Thoroughness of explanation
- **Clarity** (15%): Organization and readability

### Models Compared
- **phi4-tot**: Enhanced with Tree of Thought methodology
- **phi4-normal**: Standard prompting baseline

## ğŸŒ³ Tree of Thought Approach

The ToT model uses a structured 6-step reasoning process:

1. **Problem Analysis**: Break down components
2. **Branch Generation**: Explore multiple solution paths
3. **Path Evaluation**: Assess approach viability
4. **Path Selection**: Choose optimal strategy
5. **Step-by-Step Execution**: Implement solution
6. **Verification**: Validate final answer

## ğŸ“Š Expected Outcomes

This benchmark will help determine:
- Whether ToT prompting improves reasoning accuracy
- If structured prompting affects response times
- Which problem types benefit most from ToT
- Trade-offs between quality and efficiency

## ğŸ”§ Customization

- **Add problems**: Edit JSON files in `data/test_datasets/`
- **Modify prompts**: Update templates in `src/prompts/`
- **Adjust evaluation**: Configure weights in `config/benchmark_config.yaml`
- **Change models**: Update model definitions in `modelfiles/`

---

Ready to explore the power of structured reasoning! ğŸ§ âœ¨

## ğŸ¤ Contributing

We welcome contributions! See [CONTRIBUTING.md](CONTRIBUTING.md) for ways to help:

- ğŸ§  Add new test cases and datasets
- ğŸ”§ Improve evaluation metrics
- ğŸ¯ Experiment with prompt engineering
- ğŸ“Š Enhance visualizations
- ğŸš€ Add support for new models

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Microsoft for the Phi4-mini-reasoning model
- The Ollama team for the excellent local LLM framework
- The Tree of Thought methodology researchers
- Open source community for inspiration and tools

## ğŸ“š Citation

If you use this benchmarking suite in your research, please cite:

```
Tree of Thought Benchmarking Suite
https://github.com/bsoldate/tree-of-thought-benchmarking
```
